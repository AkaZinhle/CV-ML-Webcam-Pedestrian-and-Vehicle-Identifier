# -*- coding: utf-8 -*-
"""YoloV3 Vehicle Identification Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10oO_GmXU8YmXNRSPfnKJsuJPkN278d3g
"""

# Imports
from ultralytics import YOLO
import torch
import cv2
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import shutil
from pathlib import Path
from tqdm.auto import tqdm
import json
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
from tqdm.auto import tqdm
import shutil
from PIL import Image
import kagglehub
import random
import os
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from google.colab.patches import cv2_imshow

# Download dataset
path = kagglehub.dataset_download("klemenko/kitti-dataset")

print("Path to dataset files:", path)

# Define Directories
base_dir = Path('/root/.cache/kagglehub/datasets/klemenko/kitti-dataset/versions/1')
data_path = base_dir / 'data_object_image_2' / 'training' / 'image_2'
test_path = base_dir / 'data_object_image_2' / 'testing' / 'image_2'
label_path = base_dir / 'data_object_label_2' / 'training' / 'label_2'

# Define a dictionary mapping
classes = {
    'Car': 0,              # Label for Car
    'Pedestrian': 1,       # Label for Pedestrian
    'Van': 2,              # Label for Van
    'Cyclist': 3,          # Label for Cyclist
    'Truck': 4,            # Label for Truck
    'Misc': 5,             # Label for miscellaneous objects
    'Tram': 6,             # Label for Tram
    'Person_sitting': 7    # Label for a person sitting
}

# Generate sorted list
data = sorted(list(data_path.glob('*')))
labels = sorted(list(label_path.glob('*')))

pairs = list(zip(data, labels))
pairs[:2]

#Test Train Split
train, test = train_test_split(pairs, test_size=0.2, shuffle=True)
len(train), len(test)

# Define path
train_split_path = Path('train').resolve()
train_split_path.mkdir(exist_ok=True)
valid_path = Path('valid').resolve()
valid_path.mkdir(exist_ok=True)

#Create Yolo Annotations
def convert_kitti_to_yolo(label_file, img_width, img_height):
    """Converts KITTI label format to YOLO format."""

    with open(label_file, 'r') as f:
        lines = f.readlines()

    yolo_annotations = []

    for line in lines:
        data = line.strip().split(' ')
        class_name = data[0]

        if class_name in classes:
            class_id = classes[class_name]  # Get the class ID
            x_min = float(data[4])  # Min x
            y_min = float(data[5])  # Min y
            x_max = float(data[6])  # Max x
            y_max = float(data[7])  # Max y

            # Normalize coordinates to [0, 1]
            x_center = ((x_min + x_max) / 2) / img_width  # X center
            y_center = ((y_min + y_max) / 2) / img_height  # Y center
            width = (x_max - x_min) / img_width  # Width
            height = (y_max - y_min) / img_height  # Height

            yolo_annotations.append(f"{class_id} {x_center} {y_center} {width} {height}")

    return yolo_annotations

#Creating copy paths
for t_img, t_lb in tqdm(train):

    im_path = train_split_path / t_img.name
    lb_path = train_split_path / t_lb.name
    shutil.copy(t_img, im_path)
    shutil.copy(t_lb, lb_path)
    img = Image.open(im_path)
    img_width, img_height = img.size
    yolo_annotations = convert_kitti_to_yolo(t_lb, img_width, img_height)

    with open(lb_path, 'w') as f:
        f.write('\n'.join(yolo_annotations))

# Validating paths
for t_img, t_lb in tqdm(test):

    im_path = valid_path / t_img.name
    lb_path = valid_path / t_lb.name
    shutil.copy(t_img, im_path)
    shutil.copy(t_lb, lb_path)
    img = Image.open(im_path)
    img_width, img_height = img.size
    yolo_annotations = convert_kitti_to_yolo(t_lb, img_width, img_height)
    with open(lb_path, 'w') as f:
        f.write('\n'.join(yolo_annotations))

#Creating YAML file
yaml_file = 'names:\n'
yaml_file += '\n'.join(f'- {c}' for c in classes)
yaml_file += f'\nnc: {len(classes)}'
yaml_file += f'\ntrain: {str(train_split_path)}\nval: {str(valid_path)}'
with open('data.yaml', 'w') as f:
    f.write(yaml_file)

# Display the content of 'data.yaml' to verify its correctness
!cat data.yaml

#Load YOLOv8 Model
model = YOLO('yolov8m.yaml')
model = YOLO('yolov8m.pt')

early_stopping = EarlyStopping(
    monitor='accuracy',
    min_delta=0.01,
    patience=3,
    restore_best_weights=True,
)

with tf.device('/GPU:1'):

    epochs = 30
    train_results = model.train(
        data='data.yaml',
        epochs=epochs,
        patience=3,
        mixup=0.5,
        project='yolov8_Assignment',
    )

# Validate model
valid_results = model.val(data='data.yaml')

#Display metrics
all_metrics = valid_results.results_dict

print("\nAll Metrics:")
for key, value in all_metrics.items():
    print(f"{key}: {value}")

#Create Graphs
plt.figure(figsize=(10, 20))
plt.imshow(Image.open('yolov8_Assignment/train/results.png'))
plt.axis('off')
plt.show()

plt.figure(figsize=(10,20))
plt.imshow(Image.open('yolov8_Assignment/train2/confusion_matrix.png'))
plt.axis('off')
plt.show()

colors = {
    'Car': (255, 0, 0),          # Red
    'Pedestrian': (0, 255, 0),   # Green
    'Van': (0, 0, 255),          # Blue
    'Cyclist': (255, 255, 0),    # Cyan
    'Truck': (255, 0, 255),      # Magenta
    'Misc': (0, 255, 255),       # Yellow
    'Tram': (128, 0, 128),       # Purple
    'Person_sitting': (0, 128, 0) # Dark Green
}

image_files = os.listdir(test_path)

#Run tests on test database
num_images_to_show = 10
image_paths = random.sample([os.path.join(test_path, img) for img in image_files], min(num_images_to_show, len(image_files)))

processed_images = []

class_names = list(classes.keys())

for image_path in image_paths:

    results = model.predict(image_path)
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0]
            confidence = box.conf[0]
            class_id = int(box.cls[0])


            class_name = class_names[class_id] if class_id < len(class_names) else "Unknown"
            print(f"Class Name: {class_name}, Class ID: {class_id}, Confidence: {confidence}, BBox: [{x1}, {y1}, {x2}, {y2}]")
            label_text = f"{class_name} ({confidence:.2f})"
            color = colors.get(class_name, (255, 255, 255))
            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
            cv2.putText(img, label_text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)

    processed_images.append(img)


plt.figure(figsize=(20, 20))
for i, processed_img in enumerate(processed_images):
    plt.subplot(4, 3, i + 1)
    plt.imshow(processed_img)
    plt.axis('off')
plt.tight_layout()
plt.show()

# Function to capture a single frame from the webcam
def capture_frame(quality=0.8):
    js = Javascript('''
        async function captureFrame(quality) {
            const video = document.createElement('video');
            const stream = await navigator.mediaDevices.getUserMedia({video: true});
            video.srcObject = stream;
            await video.play();
            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            stream.getTracks().forEach(track => track.stop());
            return canvas.toDataURL('image/jpeg', quality);
        }
    ''')
    display(js)
    data = eval_js(f'captureFrame({quality})')
    binary = b64decode(data.split(',')[1])
    img_array = np.frombuffer(binary, np.uint8)
    return cv2.imdecode(img_array, cv2.IMREAD_COLOR)

# Function to perform object detection on a single frame
def detect_objects_on_frame(frame, model, labels):
    results = model(frame)
    detections = results[0].boxes.data.cpu().numpy()
    class_ids = detections[:, -1].astype(int)

    for detection in detections:
        x1, y1, x2, y2, confidence, class_id = detection
        if confidence > 0.3:

            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
            label = f"{list(labels.keys())[list(labels.values()).index(class_id)]}: {confidence * 100:.2f}%"  # Get label from class_id
            cv2.putText(frame, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    return frame

# Main function to run detection in Colab
def run_object_detection_colab(model, labels):
    print("Press 'Stop' in Colab to exit the loop.")
    try:
        while True:
            frame = capture_frame()
            if frame is None:
                print("No frame captured.")
                break

            processed_frame = detect_objects_on_frame(frame, model, labels)

            cv2_imshow(processed_frame)
    except KeyboardInterrupt:
        print("Detection stopped.")

run_object_detection_colab(model, classes)